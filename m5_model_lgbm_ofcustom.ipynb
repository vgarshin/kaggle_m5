{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from multiprocessing import Pool\n",
    "import lightgbm as lgb\n",
    "from tqdm.notebook import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "try:\n",
    "    print('available GPU devices:', len(os.environ['CUDA_VISIBLE_DEVICES']), \n",
    "          ' | device num:', os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "except:\n",
    "    print('no environ cuda device variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 80\n",
    "DATA_DIR = './data'\n",
    "MODELS_DIR = './models'\n",
    "MODEL_VER = 'v0'\n",
    "CUT_DATE = '2011-01-01'\n",
    "END_D = 1913\n",
    "PRED_FWD = 28\n",
    "N_CORES = int(psutil.cpu_count() * .75)\n",
    "print('num pf cores:', N_CORES)\n",
    "#---|CUT_DATE|---train---|END_D - PRED_FWD|--val--|END_D|--forecast-->|END_D + PRED_FWD|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALENDAR_DTYPES = {\n",
    "    'date':             'str',\n",
    "    'wm_yr_wk':         'int16', \n",
    "    'weekday':          'object',\n",
    "    'wday':             'int8', \n",
    "    'month':            'int8', \n",
    "    'year':             'int16', \n",
    "    'd':                'object',\n",
    "    'event_name_1':     'object',\n",
    "    'event_type_1':     'object',\n",
    "    'event_name_2':     'object',\n",
    "    'event_type_2':     'object',\n",
    "    'snap_CA':          'int8', \n",
    "    'snap_TX':          'int8', \n",
    "    'snap_WI':          'int8'\n",
    "}\n",
    "PARSE_DATES = ['date']\n",
    "SPRICES_DTYPES = {\n",
    "    'store_id':    'object', \n",
    "    'item_id':     'object', \n",
    "    'wm_yr_wk':    'int16',  \n",
    "    'sell_price':  'float16'\n",
    "}\n",
    "DROP_COLS = ['id', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday']\n",
    "CAT_COLS = [\n",
    "    'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', \n",
    "    'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "    'snap_CA', 'snap_TX', 'snap_WI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'BASE GRID', '-' * 10)\n",
    "    strain = pd.read_csv('{}/sales_train_validation.csv'.format(DATA_DIR))\n",
    "    print('read train:', strain.shape)\n",
    "    for day in range(END_D + 1, END_D + PRED_FWD + 1):\n",
    "        strain['d_{}'.format(day)] = np.nan\n",
    "    strain = pd.melt(\n",
    "        strain,\n",
    "        id_vars = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id'],\n",
    "        value_vars = [col for col in strain.columns if col.startswith('d_')],\n",
    "        var_name = 'd',\n",
    "        value_name = 'sales'\n",
    "    )\n",
    "    print('melted train:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    calendar = pd.read_csv('{}/calendar.csv'.format(DATA_DIR), dtype=CALENDAR_DTYPES, parse_dates=PARSE_DATES)\n",
    "    print('read calendar:', calendar.shape)\n",
    "    strain = strain.merge(calendar, on='d', copy=False)\n",
    "    strain.drop(columns=['month', 'year'], inplace=True)\n",
    "    strain['d'] = strain['d'].apply(lambda x: int(x.replace('d_', '')))  \n",
    "    print('calendar merge done:', strain.shape)\n",
    "    strain['tm_d'] = strain['date'].dt.day.astype(np.int8)\n",
    "    strain['tm_w'] = strain['date'].dt.week.astype(np.int8)\n",
    "    strain['tm_m'] = strain['date'].dt.month.astype(np.int8)\n",
    "    strain['tm_y'] = strain['date'].dt.year\n",
    "    strain['tm_y'] = (strain['tm_y'] - strain['tm_y'].min()).astype(np.int8)\n",
    "    strain['tm_wm'] = strain['tm_d'].apply(lambda x: np.ceil(x / 7)).astype(np.int8)\n",
    "    strain['tm_dw'] = strain['date'].dt.dayofweek.astype(np.int8)\n",
    "    strain['tm_w_end'] = (strain['tm_dw'] >= 5).astype(np.int8)\n",
    "    print('date features done')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    sprices = pd.read_csv('{}/sell_prices.csv'.format(DATA_DIR), dtype=SPRICES_DTYPES)\n",
    "    print('read prices:', sprices.shape)\n",
    "    release_df = sprices.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "    release_df.columns = ['store_id', 'item_id', 'release']\n",
    "    strain = strain.merge(release_df, on=['store_id', 'item_id'], how='left')\n",
    "    del release_df\n",
    "    print('release feature done')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    sprices['price_max'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('max')\n",
    "    sprices['price_min'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('min')\n",
    "    sprices['price_std'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('std')\n",
    "    sprices['price_mean'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')\n",
    "    sprices['price_norm'] = sprices['sell_price'] / sprices['price_max']\n",
    "    sprices['price_nunique'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
    "    sprices['item_nunique'] = sprices.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')\n",
    "    calendar_prices = calendar[['wm_yr_wk', 'month', 'year']]\n",
    "    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "    sprices = sprices.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')\n",
    "    del calendar, calendar_prices\n",
    "    sprices['price_momentum'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id']\n",
    "    )['sell_price'].transform(lambda x: x.shift(1))\n",
    "    sprices['price_variance_w'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'wm_yr_wk']\n",
    "    )['sell_price'].transform('std')\n",
    "    sprices['price_momentum_m'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'month']\n",
    "    )['sell_price'].transform('mean')\n",
    "    sprices['price_variance_m'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'month']\n",
    "    )['sell_price'].transform('std')\n",
    "    sprices['price_momentum_y'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'year']\n",
    "    )['sell_price'].transform('mean')\n",
    "    sprices['price_variance_y'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'year']\n",
    "    )['sell_price'].transform('std')\n",
    "    strain = strain.merge(sprices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    strain.drop(columns=['month', 'year'], inplace=True)\n",
    "    del sprices\n",
    "    print('prices features and merge done:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    for col in CAT_COLS:\n",
    "        strain[col]= strain[col].astype('category')\n",
    "        strain[col] = strain[col].cat.codes.astype('int16')\n",
    "        strain[col] -= strain[col].min()\n",
    "    print('cols to category done:', strain.shape)\n",
    "    print('begin train date:', strain['date'].min())\n",
    "    print('end train date:', strain['date'].max())\n",
    "    strain = strain.loc[strain['date'] >= CUT_DATE]\n",
    "    print('date cut train:', strain.shape)\n",
    "    print('cut train date:', strain['date'].min())\n",
    "    print('end train date:', strain['date'].max())\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    gc.collect()\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_adv(strain):\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'ADVANCED FEATURES', '-' * 10)\n",
    "    print('in dataframe:', strain.shape)\n",
    "    icols =  [\n",
    "        ['state_id'],\n",
    "        ['store_id'],\n",
    "        ['cat_id'],\n",
    "        ['dept_id'],\n",
    "        ['state_id', 'cat_id'],\n",
    "        ['state_id', 'dept_id'],\n",
    "        ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'],\n",
    "        ['item_id'],\n",
    "        ['item_id', 'state_id'],\n",
    "        ['item_id', 'store_id']\n",
    "    ]\n",
    "    cols = list(set([item for sublist in icols for item in sublist]))\n",
    "    cols.extend(['d', 'sales'])\n",
    "    df_temp = strain[cols].copy()\n",
    "    df_temp.loc[df_temp['d'] > (END_D - PRED_FWD), 'sales'] = np.nan\n",
    "    for col in icols:\n",
    "        col_name = '_{}_'.format('_'.join(col))\n",
    "        strain['enc{}mean'.format(col_name)] = df_temp.groupby(col)['sales'].transform('mean').astype(np.float16)\n",
    "        strain['enc{}std'.format(col_name)] = df_temp.groupby(col)['sales'].transform('std').astype(np.float16)\n",
    "    print('encoding done')\n",
    "    del df_temp\n",
    "    gc.collect()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_lag(strain):\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'LAG AND ROLL FEATURES', '-' * 10)\n",
    "    print('in dataframe:', strain.shape)\n",
    "    lags = range(PRED_FWD, PRED_FWD + 14 + 1)\n",
    "    lag_cols = ['lag_{}'.format(lag) for lag in lags ]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        strain[lag_col] = strain[['id', 'sales']].groupby(['id'])['sales'].shift(lag).astype(np.float16)\n",
    "    print('lag sales done')\n",
    "    for roll in [7, 14, 30, 60, 180]:\n",
    "        roll_col = 'lag_{}_roll_mean_{}'.format(PRED_FWD, roll)\n",
    "        strain[roll_col] = strain[['id', 'sales']].groupby(['id'])['sales'].transform(\n",
    "            lambda x: x.shift(PRED_FWD).rolling(roll).mean()\n",
    "        ).astype(np.float16)\n",
    "        roll_col = 'lag_{}_roll_std_{}'.format(PRED_FWD, roll)\n",
    "        strain[roll_col] = strain[['id', 'sales']].groupby(['id'])['sales'].transform(\n",
    "            lambda x: x.shift(PRED_FWD).rolling(roll).std()\n",
    "        ).astype(np.float16)\n",
    "    print('roll mean and std sales done')\n",
    "    print('out dataframe:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_last_sales(strain):\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'LAST SALES FEATURE', '-' * 10)\n",
    "    print('in dataframe:', strain.shape)\n",
    "    n_day = 1    \n",
    "    last_sales = strain[['id', 'd', 'sales']].copy()\n",
    "    last_sales['non_zero'] = (last_sales['sales'] > 0).astype(np.int8)\n",
    "    last_sales['non_zero_lag'] = last_sales.groupby(['id'])['non_zero'].transform(\n",
    "        lambda x: x.shift(n_day).rolling(2000, 1).sum()\n",
    "    ).fillna(-1)\n",
    "    df_temp = last_sales[['id', 'd', 'non_zero_lag']].drop_duplicates(subset=['id', 'non_zero_lag'])\n",
    "    df_temp.columns = ['id', 'd_min', 'non_zero_lag']\n",
    "    last_sales = last_sales.merge(df_temp, on=['id', 'non_zero_lag'], how='left')\n",
    "    strain.loc[:, 'last_sale'] = (last_sales['d'] - last_sales['d_min']).astype(np.int16)\n",
    "    del last_sales, df_temp\n",
    "    gc.collect()\n",
    "    print('last non zero sales done')\n",
    "    print('out dataframe:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_win(strain):\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'WIN ROLL FEATURES', '-' * 10)\n",
    "    print('in dataframe:', strain.shape)\n",
    "    lags = [1, 7, 14, 30]\n",
    "    windows= [7, 14, 30, 60]\n",
    "    for lag in lags:\n",
    "        for wnd in windows:\n",
    "            wnd_col = 'lag_{}_roll_mean_{}'.format(lag, wnd)\n",
    "            strain[wnd_col] = strain[['id', 'sales']].groupby(['id'])['sales'].transform(\n",
    "                lambda x: x.shift(lag).rolling(wnd).mean().astype(np.float16)\n",
    "            )\n",
    "    print('window roll mean sales done')\n",
    "    print('out dataframe:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "strain = get_df()\n",
    "strain = make_features_adv(strain)\n",
    "strain = make_features_lag(strain)\n",
    "strain = make_features_last_sales(strain)\n",
    "strain = make_features_win(strain)\n",
    "strain.to_pickle('{}/strain.pkl'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain = pd.read_pickle('{}/strain.pkl'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name = np.random.choice(strain['id'].unique())\n",
    "id_sales = strain.loc[strain['id'] == id_name].set_index('date')\n",
    "print('from', strain['date'].min(), 'to', strain['date'].max()) \n",
    "plt.figure(figsize=(18, 4))\n",
    "id_sales['sales'].plot(label='sales')\n",
    "id_sales['lag_28'].plot(label='lag_7')\n",
    "id_sales['lag_1_roll_mean_7'].plot(label='lag_1_roll_mean_7')\n",
    "plt.title(id_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = strain.columns[~strain.columns.isin(DROP_COLS)]\n",
    "store_ids = list(strain['store_id'].unique())\n",
    "print('stores:', store_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val(strain, val_d, end_d):\n",
    "    X_train = strain[(strain['d'] <= end_d) & \n",
    "                     (strain['store_id'] == store_id)][train_cols]\n",
    "    y_train = strain[(strain['d'] <= end_d) & \n",
    "                     (strain['store_id'] == store_id)]['sales']\n",
    "    X_val = strain[(strain['d'] > (val_d - PRED_FWD)) & \n",
    "                   (strain['d'] <= end_d) & \n",
    "                   (strain['store_id'] == store_id)][train_cols]\n",
    "    X_val = X_val.append(strain[(strain['d'] > end_d - 365) & \n",
    "                                (strain['d'] <= end_d + PRED_FWD - 365) & \n",
    "                                (strain['store_id'] == store_id)][train_cols])\n",
    "    y_val = strain[(strain['d'] > (val_d - PRED_FWD)) & \n",
    "                   (strain['d'] <= end_d) & \n",
    "                   (strain['store_id'] == store_id)]['sales']\n",
    "    y_val = y_val.append(strain[(strain['d'] > end_d - 365) & \n",
    "                                (strain['d'] <= end_d + PRED_FWD - 365) & \n",
    "                                (strain['store_id'] == store_id)]['sales'])\n",
    "    return X_train, y_train, X_val, y_val\n",
    "def custom_asymmetric_train(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype('float')\n",
    "    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n",
    "    hess = np.where(residual < 0, 2, 2 * 1.15)\n",
    "    return grad, hess\n",
    "def custom_asymmetric_valid(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype('float')\n",
    "    loss = np.where(residual < 0, (residual ** 2) , (residual ** 2) * 1.15) \n",
    "    return 'custom_asymmetric_eval', np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM = False\n",
    "if CUSTOM:\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'nthread': N_CORES,\n",
    "        'seed': SEED,\n",
    "        'learning_rate': .03,\n",
    "        'subsample': .5,\n",
    "        'subsample_freq': 1,\n",
    "        'num_leaves': 2 ** 11 - 1,\n",
    "        'min_data_in_leaf': 2 ** 12 - 1,\n",
    "        'feature_fraction': .5,\n",
    "        'max_bin': 100,\n",
    "        'n_estimators': 1400,\n",
    "        'boost_from_average': False,\n",
    "        'verbose': 1,\n",
    "        #'early_stopping_rounds': 50,\n",
    "        'lambda_l2': 1\n",
    "    }\n",
    "else:\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'subsample': .5,\n",
    "        'subsample_freq': 1,\n",
    "        'learning_rate': .03,\n",
    "        'num_leaves': 2 ** 11 - 1,\n",
    "        'min_data_in_leaf': 2 ** 12 - 1,\n",
    "        'feature_fraction': .5,\n",
    "        'max_bin': 100,\n",
    "        'n_estimators': 2000,\n",
    "        'boost_from_average': False,\n",
    "        'verbose': 1,\n",
    "        'nthread' : N_CORES,\n",
    "        #'early_stopping_rounds': 50,\n",
    "        #'lambda_l2': 1,\n",
    "        'seed': SEED\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for store_id in tqdm(store_ids):\n",
    "    print('-' * 10, 'store training:', store_id, '-' * 10)\n",
    "    X_train, y_train, X_val, y_val = get_train_val(\n",
    "        strain, \n",
    "        val_d=END_D - PRED_FWD,\n",
    "        end_d=END_D\n",
    "    )\n",
    "    print('train shapes:', X_train.shape, len(y_train))\n",
    "    print('val shapes:', X_val.shape, len(y_val))\n",
    "    train_lgb = lgb.Dataset(X_train, label=y_train, categorical_feature=CAT_COLS, free_raw_data=False)\n",
    "    val_lgb = lgb.Dataset(X_val, label=y_val, categorical_feature=CAT_COLS, free_raw_data=False)\n",
    "    del X_train, y_train, X_val, y_val\n",
    "    gc.collect()\n",
    "    if CUSTOM:\n",
    "        model = lgb.train(lgb_params, train_lgb, valid_sets=[val_lgb], verbose_eval=200,\n",
    "                          fobj=custom_asymmetric_train, feval=custom_asymmetric_valid) \n",
    "    else:\n",
    "        model = lgb.train(lgb_params, train_lgb, valid_sets=[val_lgb], verbose_eval=200) \n",
    "    model_file = '{}/model_{}_store_{}.lgb'.format(MODELS_DIR, MODEL_VER, store_id)\n",
    "    model.save_model(model_file)\n",
    "    print('save to file:', model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id = 0\n",
    "model_file = '{}/model_{}_store_{}.lgb'.format(MODELS_DIR, MODEL_VER, store_id)\n",
    "model = lgb.Booster(model_file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = sorted(\n",
    "    [(f, v) for f, v in zip(train_cols, model.feature_importance())],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "labels, values = [x[0] for x in feat_importances[:20]], [x[1] for x in feat_importances[:20]]\n",
    "y_pos = np.arange(len(labels))\n",
    "ax.barh(y_pos, values)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Performance')\n",
    "ax.set_title('feature importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spred = strain[\n",
    "    (strain['d'] > (END_D - PRED_FWD)) &\n",
    "    (strain['d'] <= END_D) &\n",
    "    (strain['store_id'] == store_id)\n",
    "].copy()\n",
    "preds = model.predict(spred[train_cols])\n",
    "print(len(preds))\n",
    "spred.loc[:, 'sales'] = np.where(preds <= .5, 0, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_name = np.random.choice(strain[strain['store_id'] == store_id]['id'].unique())\n",
    "print('id to draw:', id_name)\n",
    "id_sales = strain.loc[\n",
    "    (strain['id'] == id_name) & \n",
    "    (strain['d'] > (END_D - 2 * PRED_FWD)) &\n",
    "    (strain['d'] <= END_D) &\n",
    "    (strain['store_id'] == store_id)\n",
    "].set_index('date')\n",
    "id_sales_pred = spred.loc[spred['id'] == id_name].set_index('date')\n",
    "plt.figure(figsize=(18, 4))\n",
    "id_sales['sales'].plot(label='sales')\n",
    "id_sales_pred['sales'].plot(label='sales pred')\n",
    "plt.title(id_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pred_d in tqdm(range(1, PRED_FWD + 1)):\n",
    "    print('=' * 10, 'forecast day forward:', pred_d, '=' * 10) \n",
    "    strain = make_features_last_sales(strain)\n",
    "    spred = strain[strain['d'] > (END_D - 100)].copy()\n",
    "    spred = make_features_win(spred)\n",
    "    for store_id in store_ids:\n",
    "        model_file = '{}/model_{}_store_{}.lgb'.format(MODELS_DIR, MODEL_VER, store_id)\n",
    "        model = lgb.Booster(model_file=model_file)\n",
    "        preds = model.predict(\n",
    "            spred.loc[\n",
    "                (spred['d'] == (END_D + pred_d)) & (spred['store_id'] == store_id), \n",
    "                train_cols\n",
    "            ]\n",
    "        )\n",
    "        strain.loc[\n",
    "            (strain['d'] == (END_D + pred_d)) & (strain['store_id'] == store_id), \n",
    "            'sales'\n",
    "        ] = np.where(preds <= .1, 0, preds)\n",
    "        print('store predicted:', store_id, '| model:', model_file)\n",
    "    all_sales = strain[strain['d'] == (END_D + pred_d)]['sales'].values\n",
    "    print('day forward:', END_D + pred_d, 'all sales:', np.sum(all_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name = np.random.choice(strain['id'].unique())\n",
    "id_sales = strain.loc[\n",
    "    (strain['id'] == id_name) & \n",
    "    (strain['d'] > (END_D - 2 * PRED_FWD)) &\n",
    "    (strain['d'] <= END_D)\n",
    "].set_index('date')\n",
    "id_sales_pred = strain.loc[\n",
    "    (strain['id'] == id_name) & \n",
    "    (strain['d'] > END_D)\n",
    "].set_index('date')\n",
    "plt.figure(figsize=(18, 4))\n",
    "id_sales['sales'].plot(label='sales')\n",
    "id_sales_pred['sales'].plot(label='sales prediction')\n",
    "plt.title(id_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spred_subm = strain.loc[strain['d'] > END_D, ['id', 'd', 'sales']].copy()\n",
    "spred_subm['d'] = spred_subm['d'].apply(lambda x: 'F{}'.format(x - END_D))\n",
    "spred_subm.loc[spred_subm['sales'] < 0, 'sales'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = ['F{}'.format(x) for x in range(1, PRED_FWD + 1)]\n",
    "spred_subm = spred_subm.set_index(['id', 'd']).unstack()['sales'][f_cols].reset_index()\n",
    "spred_subm.fillna(0, inplace=True)\n",
    "spred_subm.sort_values('id', inplace=True)\n",
    "spred_subm.reset_index(drop=True, inplace=True)\n",
    "spred_subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spred_subm_eval = spred_subm.copy()\n",
    "spred_subm_eval['id'] = spred_subm_eval['id'].str.replace('validation', 'evaluation')\n",
    "spred_subm = pd.concat([spred_subm, spred_subm_eval], axis=0, sort=False)\n",
    "spred_subm.reset_index(drop=True, inplace=True)\n",
    "spred_subm.to_csv('submission.csv', index=False)\n",
    "print('submission saved:', spred_subm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
