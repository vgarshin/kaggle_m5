{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available GPU devices: 1  | device num: 0\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "pd.set_option('display.max_columns', None)\n",
    "print('available GPU devices:', len(os.environ['CUDA_VISIBLE_DEVICES']), \n",
    "      ' | device num:', os.environ['CUDA_VISIBLE_DEVICES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-04-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "MODELS_DIR = './models'\n",
    "MODEL_VER = 'v0'\n",
    "CUT_DATE = '2015-10-24'\n",
    "END_DATE = '2016-04-24'\n",
    "print(datetime.strptime(END_DATE, '%Y-%m-%d'))\n",
    "LOOK_BACK = 2 * 28\n",
    "LOOK_FWD = 28\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALENDAR_DTYPES = {\n",
    "    'date':             'str',\n",
    "    'wm_yr_wk':         'int16', \n",
    "    'weekday':          'object',\n",
    "    'wday':             'int16', \n",
    "    'month':            'int16', \n",
    "    'year':             'int16', \n",
    "    'd':                'object',\n",
    "    'event_name_1':     'object',\n",
    "    'event_type_1':     'object',\n",
    "    'event_name_2':     'object',\n",
    "    'event_type_2':     'object',\n",
    "    'snap_CA':          'int16', \n",
    "    'snap_TX':          'int16', \n",
    "    'snap_WI':          'int16'\n",
    "}\n",
    "PARSE_DATES = ['date']\n",
    "SPRICES_DTYPES = {\n",
    "    'store_id':    'object', \n",
    "    'item_id':     'object', \n",
    "    'wm_yr_wk':    'int16',  \n",
    "    'sell_price':  'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(is_train=True, backward_lags=None):\n",
    "    strain = pd.read_csv('{}/sales_train_validation.csv'.format(DATA_DIR))\n",
    "    print('read train:', strain.shape)\n",
    "    cat_cols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    last_day = int(strain.columns[-1].replace('d_', ''))\n",
    "    print('last day is:', last_day)\n",
    "    if not is_train:\n",
    "        for day in range(last_day + 1, last_day + 28 + 28 + 1):\n",
    "            strain['d_{}'.format(day)] = np.nan\n",
    "    strain = pd.melt(\n",
    "        strain,\n",
    "        id_vars = cat_cols,\n",
    "        value_vars = [col for col in strain.columns if col.startswith('d_')],\n",
    "        var_name = 'd',\n",
    "        value_name = 'sales'\n",
    "    )\n",
    "    print('melted train:', strain.shape)\n",
    "    calendar = pd.read_csv('{}/calendar.csv'.format(DATA_DIR), dtype=CALENDAR_DTYPES, parse_dates=PARSE_DATES)\n",
    "    print('read calendar:', calendar.shape)\n",
    "    strain = strain.merge(calendar, on='d', copy=False)\n",
    "    print('calendar merge done')\n",
    "    sprices = pd.read_csv('{}/sell_prices.csv'.format(DATA_DIR), dtype=SPRICES_DTYPES)\n",
    "    print('read prices:', sprices.shape)\n",
    "    strain = strain.merge(\n",
    "        sprices, \n",
    "        on=['store_id', 'item_id', 'wm_yr_wk'], \n",
    "        copy=False\n",
    "    )\n",
    "    print('prices merge done')\n",
    "    print('begin train date:', strain['date'].min())\n",
    "    print('end train date:', strain['date'].max())\n",
    "    if not is_train:\n",
    "        strain = strain.loc[\n",
    "            strain['date'] >= (datetime.strptime(END_DATE, '%Y-%m-%d') - timedelta(days=backward_lags))\n",
    "        ]\n",
    "    else:\n",
    "        strain = strain.loc[strain['date'] >= CUT_DATE]\n",
    "    print('date cut train:', strain.shape)\n",
    "    print('cut train date:', strain['date'].min())\n",
    "    print('end train date:', strain['date'].max())\n",
    "    date_features = {\n",
    "        'week_num': 'weekofyear',\n",
    "        'quarter': 'quarter',\n",
    "        'mday': 'day'\n",
    "    }\n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        strain[date_feat_name] = getattr(strain['date'].dt, date_feat_func).astype('int16')\n",
    "    print('date features done')\n",
    "    strain['d'] = strain['d'].apply(lambda x: int(x.replace('d_', '')))  \n",
    "    drop_cols = ['date', 'wm_yr_wk', 'weekday']\n",
    "    strain.drop(columns=drop_cols, inplace=True)\n",
    "    strain.sort_values(by=['id', 'd'], inplace=True)\n",
    "    print('trash cols deleted, sorted')\n",
    "    strain['d_'] = strain['d']\n",
    "    print('out dataframe:', strain.shape)\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(strain, cols_dummy):\n",
    "    print('got shape for dummies:', strain.shape)\n",
    "    strain_dummies = pd.get_dummies(\n",
    "        strain[cols_dummy],\n",
    "        drop_first=False,\n",
    "        dummy_na=True\n",
    "    )\n",
    "    strain.drop(columns=cols_dummy, inplace=True)\n",
    "    strain = pd.concat([strain, strain_dummies], axis=1)\n",
    "    print('out shape for dummies:', strain.shape)\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train: (30490, 1919)\n",
      "last day is: 1913\n",
      "melted train: (58327370, 8)\n",
      "read calendar: (1969, 14)\n",
      "calendar merge done\n",
      "read prices: (6841121, 4)\n",
      "prices merge done\n",
      "begin train date: 2011-01-29 00:00:00\n",
      "end train date: 2016-04-24 00:00:00\n",
      "date cut train: (5607717, 22)\n",
      "cut train date: 2015-10-24 00:00:00\n",
      "end train date: 2016-04-24 00:00:00\n",
      "date features done\n",
      "trash cols deleted, sorted\n",
      "out dataframe: (5607717, 23)\n",
      "CPU times: user 40.8 s, sys: 6.6 s, total: 47.4 s\n",
      "Wall time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "strain = get_df(is_train=True, backward_lags=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min-max scaled\n"
     ]
    }
   ],
   "source": [
    "num_cols = ['d', 'sales', 'wday', 'month', 'year', \n",
    "            'snap_CA', 'snap_TX', 'snap_WI', \n",
    "            'sell_price', 'week_num', 'quarter', 'mday']\n",
    "scaler = MinMaxScaler()\n",
    "strain[num_cols] = scaler.fit_transform(strain[num_cols])\n",
    "print('min-max scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | start: 1829 end: 1913 duration: 84 deep: 84\n",
      "train | start: 1730 end: 1885 duration: 155 deep: 84\n",
      "dummies: id (30490,)\n",
      "dummies: item_id (3049,)\n",
      "dummies: dept_id (7,)\n",
      "dummies: store_id (10,)\n",
      "dummies: cat_id (3,)\n",
      "dummies: state_id (3,)\n"
     ]
    }
   ],
   "source": [
    "sval = strain[strain.d_ >= (1913 - LOOK_BACK - LOOK_FWD)]\n",
    "print('val | start:', sval.d_.min(), \n",
    "      'end:', sval.d_.max(),\n",
    "      'duration:', sval.d_.max() - sval.d_.min(), \n",
    "      'deep:', LOOK_BACK + LOOK_FWD)\n",
    "strain = strain[strain.d_ <= (1913 - LOOK_FWD)]\n",
    "print('train | start:', strain.d_.min(), \n",
    "      'end:', strain.d_.max(),\n",
    "      'duration:', strain.d_.max() - strain.d_.min(), \n",
    "      'deep:', LOOK_BACK + LOOK_FWD)\n",
    "event_cols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "sdummies_id = strain.drop(columns=num_cols + event_cols + ['d_'])\n",
    "strain.drop(columns=['d_'], inplace=True)\n",
    "sval.drop(columns=['d_'], inplace=True)\n",
    "sdummies_id.drop_duplicates(inplace=True)\n",
    "print('dummies df done:', sdummies_id.shape)\n",
    "for col in sdummies_id.columns:\n",
    "    print('dummies:', col, sdummies_id[col].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got shape for dummies: (30490, 6)\n",
      "out shape for dummies: (30490, 3078)\n",
      "CPU times: user 168 ms, sys: 4 ms, total: 172 ms\n",
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols_dummy = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id']\n",
    "sdummies_id = get_dummies(sdummies_id, cols_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_batch(df, dummies, col_id, col_look, look_back, look_fwd, cols_features):\n",
    "    X, y = [], [] \n",
    "    for idx in list(df['id'].unique()):\n",
    "        for i in range(len(df.loc[df[col_id] == idx, col_look]) - look_back - look_fwd):\n",
    "            temp_X = []\n",
    "            temp_X.append(df.loc[df[col_id] == idx, col_look][i : i + look_back])\n",
    "            for col in cols_features:\n",
    "                temp_X.append(df.loc[df[col_id] == idx, col][i + look_fwd : i + look_back + look_fwd])\n",
    "            temp_D = np.array([dummies.loc[dummies[col_id] == idx].values[0][1:]] * look_back).T\n",
    "            temp_X = np.vstack((temp_X, temp_D))\n",
    "            X.append(temp_X)\n",
    "            y.append(df.loc[df[col_id] == idx, col_look][i + look_back : i + look_back + look_fwd])\n",
    "    X = [x.T for x in np.array(X)] # to feed LSTM with shape as [samples, time steps, features]\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeqGenerator(df, dummies, col_id, col_look, look_back, look_fwd, cols_features, batch_size):\n",
    "    while True:\n",
    "        for i in range(len(df) // batch_size):\n",
    "            if (i + 1) * batch_size > len(df):\n",
    "                yield get_sequence_batch(\n",
    "                    df[i * batch_size:], \n",
    "                    dummies,\n",
    "                    col_id, \n",
    "                    col_look, \n",
    "                    look_back, \n",
    "                    look_fwd, \n",
    "                    cols_features\n",
    "                )\n",
    "            else:\n",
    "                yield get_sequence_batch(\n",
    "                    df[i * batch_size : (i + 1) * batch_size], \n",
    "                    dummies,\n",
    "                    col_id, \n",
    "                    col_look, \n",
    "                    look_back, \n",
    "                    look_fwd, \n",
    "                    cols_features\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(432, 56, 3090) (432, 28)\n",
      "CPU times: user 3.49 s, sys: 320 ms, total: 3.81 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = next(\n",
    "    SeqGenerator(\n",
    "        df=strain, \n",
    "        dummies=sdummies_id, \n",
    "        col_id='id', \n",
    "        col_look='sales', \n",
    "        look_back=LOOK_BACK, \n",
    "        look_fwd=LOOK_FWD, \n",
    "        cols_features=num_cols, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    ")\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches in train: 4753\n",
      "batches in val: 2591\n"
     ]
    }
   ],
   "source": [
    "print('batches in train:', len(strain) // BATCH_SIZE + 1)\n",
    "print('batches in val:', len(sval) // BATCH_SIZE + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/mrorangeenv/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 56, 512)           7378944   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 28)                14364     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28)                0         \n",
      "=================================================================\n",
      "Total params: 9,492,508\n",
      "Trainable params: 9,492,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features = 3090\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=512, input_shape=(LOOK_BACK, n_features), return_sequences=True))\n",
    "model.add(Dropout(.4))\n",
    "model.add(LSTM(units=512))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Dense(LOOK_FWD))\n",
    "model.add(Activation('linear'))\n",
    "adam = optimizers.Adam(lr=.001, clipvalue=.5, clipnorm=1)\n",
    "model.compile(loss='mse', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /opt/conda/envs/mrorangeenv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "4753/4754 [============================>.] - ETA: 5s - loss: 1.2342e-04 \n",
      "Epoch 00001: val_loss improved from inf to 0.00016, saving model to ./models/model_v0.h5\n",
      "4754/4754 [==============================] - 26023s 5s/step - loss: 1.2340e-04 - val_loss: 1.6094e-04\n",
      "Epoch 2/100\n",
      "2639/4754 [===============>..............] - ETA: 3:09:49 - loss: 1.7209e-04"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_file = '{}/model_{}.h5'.format(MODELS_DIR, MODEL_VER)\n",
    "modelsaver = ModelCheckpoint(\n",
    "    model_file, \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "history = model.fit_generator(\n",
    "    SeqGenerator(\n",
    "        df=strain, \n",
    "        dummies=sdummies_id, \n",
    "        col_id='id', \n",
    "        col_look='sales', \n",
    "        look_back=LOOK_BACK, \n",
    "        look_fwd=LOOK_FWD, \n",
    "        cols_features=num_cols, \n",
    "        batch_size=BATCH_SIZE\n",
    "    ),\n",
    "    steps_per_epoch=len(strain) // BATCH_SIZE + 1,\n",
    "    validation_data=SeqGenerator(\n",
    "        df=sval, \n",
    "        dummies=sdummies_id, \n",
    "        col_id='id', \n",
    "        col_look='sales', \n",
    "        look_back=LOOK_BACK, \n",
    "        look_fwd=LOOK_FWD, \n",
    "        cols_features=num_cols, \n",
    "        batch_size=BATCH_SIZE\n",
    "    ),\n",
    "    validation_steps=len(sval) // BATCH_SIZE + 1,\n",
    "    epochs=100,\n",
    "    callbacks=[earlystopper, modelsaver],\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
