{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from multiprocessing import Pool\n",
    "pd.set_option('display.max_columns', None)\n",
    "print('available GPU devices:', len(os.environ['CUDA_VISIBLE_DEVICES']), \n",
    "      '| device num:', os.environ['CUDA_VISIBLE_DEVICES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 80\n",
    "DATA_DIR = './data_upd'\n",
    "MODELS_DIR = './models'\n",
    "MODEL_VER = 'v8'\n",
    "CUT_DATE = '2011-01-01'\n",
    "END_D = 1941\n",
    "PRED_FWD = 28\n",
    "N_CORES = int(psutil.cpu_count() * .75)\n",
    "print('num of cores:', N_CORES)\n",
    "N_FOLDS = 4\n",
    "#---|CUT_DATE|---train---|END_D - PRED_FWD|--val--|END_D|--forecast-->|END_D + PRED_FWD|\n",
    "# v1 4 folds years months --> 0.48209\n",
    "# v4 4 folds years months earlystop --> 0.47588 / no zeros less .1 --> 0.49578\n",
    "# v5 5 folds years months earlystop zeros less .5 --> 0.8779\n",
    "# v6 3 folds years months earlystop zeros less .1 --> 0.4811\n",
    "# v7 4 folds years months earlystop BASELINE --> 0.4777\n",
    "# v8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALENDAR_DTYPES = {\n",
    "    'date':             'str',\n",
    "    'wm_yr_wk':         'int16', \n",
    "    'weekday':          'object',\n",
    "    'wday':             'int8', \n",
    "    'month':            'int8', \n",
    "    'year':             'int16', \n",
    "    'd':                'object',\n",
    "    'event_name_1':     'object',\n",
    "    'event_type_1':     'object',\n",
    "    'event_name_2':     'object',\n",
    "    'event_type_2':     'object',\n",
    "    'snap_CA':          'int8', \n",
    "    'snap_TX':          'int8', \n",
    "    'snap_WI':          'int8'\n",
    "}\n",
    "PARSE_DATES = ['date']\n",
    "SPRICES_DTYPES = {\n",
    "    'store_id':    'object', \n",
    "    'item_id':     'object', \n",
    "    'wm_yr_wk':    'int16',  \n",
    "    'sell_price':  'float16'\n",
    "}\n",
    "DROP_COLS = ['id', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday']\n",
    "CAT_COLS = [\n",
    "    'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', \n",
    "    'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "    'snap_CA', 'snap_TX', 'snap_WI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'BASE GRID', '-' * 10)\n",
    "    strain = pd.read_csv('{}/sales_train_evaluation.csv'.format(DATA_DIR))\n",
    "    print('read train:', strain.shape)\n",
    "    for day in range(END_D + 1, END_D + PRED_FWD + 1):\n",
    "        strain['d_{}'.format(day)] = np.nan\n",
    "    strain = pd.melt(\n",
    "        strain,\n",
    "        id_vars = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id'],\n",
    "        value_vars = [col for col in strain.columns if col.startswith('d_')],\n",
    "        var_name = 'd',\n",
    "        value_name = 'sales'\n",
    "    )\n",
    "    print('melted train:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    calendar = pd.read_csv('{}/calendar.csv'.format(DATA_DIR), dtype=CALENDAR_DTYPES, parse_dates=PARSE_DATES)\n",
    "    print('read calendar:', calendar.shape)\n",
    "    strain = strain.merge(calendar, on='d', copy=False)\n",
    "    strain.drop(columns=['month', 'year'], inplace=True)\n",
    "    strain['d'] = strain['d'].apply(lambda x: int(x.replace('d_', '')))  \n",
    "    print('calendar merge done:', strain.shape)\n",
    "    strain['tm_d'] = strain['date'].dt.day.astype(np.int8)\n",
    "    strain['tm_w'] = strain['date'].dt.week.astype(np.int8)\n",
    "    strain['tm_m'] = strain['date'].dt.month.astype(np.int8)\n",
    "    strain['tm_y'] = strain['date'].dt.year\n",
    "    strain['tm_y'] = (strain['tm_y'] - strain['tm_y'].min()).astype(np.int8)\n",
    "    strain['tm_wm'] = strain['tm_d'].apply(lambda x: np.ceil(x / 7)).astype(np.int8)\n",
    "    strain['tm_dw'] = strain['date'].dt.dayofweek.astype(np.int8)\n",
    "    strain['tm_w_end'] = (strain['tm_dw'] >= 5).astype(np.int8)\n",
    "    print('date features done')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    sprices = pd.read_csv('{}/sell_prices.csv'.format(DATA_DIR), dtype=SPRICES_DTYPES)\n",
    "    print('read prices:', sprices.shape)\n",
    "    release_df = sprices.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "    release_df.columns = ['store_id', 'item_id', 'release']\n",
    "    strain = strain.merge(release_df, on=['store_id', 'item_id'], how='left')\n",
    "    del release_df\n",
    "    print('release feature done')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    sprices['price_max'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('max')\n",
    "    sprices['price_min'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('min')\n",
    "    sprices['price_std'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('std')\n",
    "    sprices['price_mean'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')\n",
    "    sprices['price_norm'] = sprices['sell_price'] / sprices['price_max']\n",
    "    sprices['price_nunique'] = sprices.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
    "    sprices['item_nunique'] = sprices.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')\n",
    "    calendar_prices = calendar[['wm_yr_wk', 'month', 'year']]\n",
    "    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "    sprices = sprices.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')\n",
    "    del calendar, calendar_prices\n",
    "    sprices['price_momentum'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id']\n",
    "    )['sell_price'].transform(lambda x: x.shift(1))\n",
    "    sprices['price_variance_w'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'wm_yr_wk']\n",
    "    )['sell_price'].transform('std')\n",
    "    sprices['price_momentum_m'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'month']\n",
    "    )['sell_price'].transform('mean')\n",
    "    sprices['price_variance_m'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'month']\n",
    "    )['sell_price'].transform('std')\n",
    "    sprices['price_momentum_y'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'year']\n",
    "    )['sell_price'].transform('mean')\n",
    "    sprices['price_variance_y'] = sprices['sell_price'] / sprices.groupby(\n",
    "        ['store_id', 'item_id', 'year']\n",
    "    )['sell_price'].transform('std')\n",
    "    strain = strain.merge(sprices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    strain.drop(columns=['month', 'year'], inplace=True)\n",
    "    del sprices\n",
    "    print('prices features and merge done:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    for col in CAT_COLS:\n",
    "        strain[col]= strain[col].astype('category')\n",
    "        strain[col] = strain[col].cat.codes.astype('int16')\n",
    "        strain[col] -= strain[col].min()\n",
    "    print('cols to category done:', strain.shape)\n",
    "    print('begin train date:', strain['date'].min())\n",
    "    print('end train date:', strain['date'].max())\n",
    "    strain = strain.loc[strain['date'] >= CUT_DATE]\n",
    "    print('date cut train:', strain.shape)\n",
    "    print('cut train date:', strain['date'].min())\n",
    "    print('end train date:', strain['date'].max())\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    gc.collect()\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_adv(strain):\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'ADVANCED FEATURES', '-' * 10)\n",
    "    print('in dataframe:', strain.shape)\n",
    "    icols =  [\n",
    "        ['state_id'],\n",
    "        ['store_id'],\n",
    "        ['cat_id'],\n",
    "        ['dept_id'],\n",
    "        ['state_id', 'cat_id'],\n",
    "        ['state_id', 'dept_id'],\n",
    "        ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'],\n",
    "        ['item_id'],\n",
    "        ['item_id', 'state_id'],\n",
    "        ['item_id', 'store_id']\n",
    "    ]\n",
    "    cols = list(set([item for sublist in icols for item in sublist]))\n",
    "    cols.extend(['d', 'sales'])\n",
    "    df_temp = strain[cols].copy()\n",
    "    df_temp.loc[df_temp['d'] > (END_D - PRED_FWD), 'sales'] = np.nan\n",
    "    for col in icols:\n",
    "        col_name = '_{}_'.format('_'.join(col))\n",
    "        strain['enc{}mean'.format(col_name)] = df_temp.groupby(col)['sales'].transform('mean').astype(np.float16)\n",
    "        strain['enc{}std'.format(col_name)] = df_temp.groupby(col)['sales'].transform('std').astype(np.float16)\n",
    "    print('encoding done')\n",
    "    del df_temp\n",
    "    gc.collect()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_lag(strain):\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'LAG AND ROLL FEATURES', '-' * 10)\n",
    "    print('in dataframe:', strain.shape)\n",
    "    lags = range(PRED_FWD, PRED_FWD + 14 + 1)\n",
    "    lag_cols = ['lag_{}'.format(lag) for lag in lags ]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        strain[lag_col] = strain[['id', 'sales']].groupby(['id'])['sales'].shift(lag).astype(np.float16)\n",
    "    print('lag sales done')\n",
    "    for roll in [7, 14, 30, 60, 180]:\n",
    "        roll_col = 'lag_{}_roll_mean_{}'.format(PRED_FWD, roll)\n",
    "        strain[roll_col] = strain[['id', 'sales']].groupby(['id'])['sales'].transform(\n",
    "            lambda x: x.shift(PRED_FWD).rolling(roll).mean()\n",
    "        ).astype(np.float16)\n",
    "        roll_col = 'lag_{}_roll_std_{}'.format(PRED_FWD, roll)\n",
    "        strain[roll_col] = strain[['id', 'sales']].groupby(['id'])['sales'].transform(\n",
    "            lambda x: x.shift(PRED_FWD).rolling(roll).std()\n",
    "        ).astype(np.float16)\n",
    "    print('roll mean and std sales done')\n",
    "    print('out dataframe:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_last_sales(strain):\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'LAST SALES FEATURE', '-' * 10)\n",
    "    print('in dataframe:', strain.shape)\n",
    "    n_day = 1    \n",
    "    last_sales = strain[['id', 'd', 'sales']].copy()\n",
    "    last_sales['non_zero'] = (last_sales['sales'] > 0).astype(np.int8)\n",
    "    last_sales['non_zero_lag'] = last_sales.groupby(['id'])['non_zero'].transform(\n",
    "        lambda x: x.shift(n_day).rolling(2000, 1).sum()\n",
    "    ).fillna(-1)\n",
    "    df_temp = last_sales[['id', 'd', 'non_zero_lag']].drop_duplicates(subset=['id', 'non_zero_lag'])\n",
    "    df_temp.columns = ['id', 'd_min', 'non_zero_lag']\n",
    "    last_sales = last_sales.merge(df_temp, on=['id', 'non_zero_lag'], how='left')\n",
    "    strain.loc[:, 'last_sale'] = (last_sales['d'] - last_sales['d_min']).astype(np.int16)\n",
    "    del last_sales, df_temp\n",
    "    gc.collect()\n",
    "    print('last non zero sales done')\n",
    "    print('out dataframe:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_win(strain):\n",
    "    start_time = time.time()\n",
    "    print('-' * 10, 'WIN ROLL FEATURES', '-' * 10)\n",
    "    print('in dataframe:', strain.shape)\n",
    "    lags = [1, 7, 14, 30]\n",
    "    windows= [7, 14, 30, 60]\n",
    "    for lag in lags:\n",
    "        for wnd in windows:\n",
    "            wnd_col = 'lag_{}_roll_mean_{}'.format(lag, wnd)\n",
    "            strain[wnd_col] = strain[['id', 'sales']].groupby(['id'])['sales'].transform(\n",
    "                lambda x: x.shift(lag).rolling(wnd).mean().astype(np.float16)\n",
    "            )\n",
    "    print('window roll mean sales done')\n",
    "    print('out dataframe:', strain.shape)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time elapsed: {:.0f} min {:.0f} sec'.format(elapsed_time // 60, elapsed_time % 60))\n",
    "    return strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CREATE_TRAIN = False\n",
    "if CREATE_TRAIN:\n",
    "    strain = get_df()\n",
    "    strain = make_features_adv(strain)\n",
    "    strain = make_features_lag(strain)\n",
    "    strain = make_features_last_sales(strain)\n",
    "    strain = make_features_win(strain)\n",
    "    strain.to_pickle('{}/strain.pkl'.format(DATA_DIR))\n",
    "    print('pickled')\n",
    "else:\n",
    "    strain = pd.read_pickle('{}/strain.pkl'.format(DATA_DIR))\n",
    "    print(strain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name = np.random.choice(strain['id'].unique())\n",
    "id_sales = strain.loc[strain['id'] == id_name].set_index('date')\n",
    "print('from', strain['date'].min(), 'to', strain['date'].max()) \n",
    "plt.figure(figsize=(18, 4))\n",
    "id_sales['sales'].plot(label='sales')\n",
    "id_sales['lag_28'].plot(label='lag_28')\n",
    "id_sales['lag_28_roll_mean_7'].plot(label='lag_28_roll_mean_7')\n",
    "plt.title(id_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = strain.columns[~strain.columns.isin(DROP_COLS)]\n",
    "store_ids = list(strain['store_id'].unique())\n",
    "print('stores:', store_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'tweedie',\n",
    "    'tweedie_variance_power': 1.1,\n",
    "    'metric': 'rmse',\n",
    "    'subsample': .5,\n",
    "    'subsample_freq': 1,\n",
    "    'learning_rate': .03,\n",
    "    'num_leaves': 2 ** 11 - 1,\n",
    "    'min_data_in_leaf': 2 ** 12 - 1,\n",
    "    'feature_fraction': .5,\n",
    "    'max_bin': 100,\n",
    "    'n_estimators': 5000,\n",
    "    'boost_from_average': False,\n",
    "    'verbose': 1,\n",
    "    'nthread' : N_CORES,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'lambda_l2': .1,\n",
    "    'seed': SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SHIFT = PRED_FWD # = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for store_id in tqdm(store_ids):\n",
    "    print('=' * 10, 'store training:', store_id, '=' * 10)\n",
    "    X = strain[(strain['d'] <= (END_D - VAL_SHIFT)) & (strain['store_id'] == store_id)][train_cols]\n",
    "    y = strain[(strain['d'] <= (END_D - VAL_SHIFT)) & (strain['store_id'] == store_id)]['sales']\n",
    "    folds = GroupKFold(n_splits=N_FOLDS)\n",
    "    X['groups'] = X['tm_w'].astype(str) + '_' + X['tm_y'].astype(str)\n",
    "    split_groups = X['groups']\n",
    "    for fold_num, (train_ids, val_ids) in enumerate(folds.split(X, y, groups=split_groups)):\n",
    "        print('-' * 10, 'fold training:', fold_num, '-' * 10)\n",
    "        X_train, y_train = X[train_cols].iloc[train_ids, :], y.iloc[train_ids]\n",
    "        X_val, y_val = X[train_cols].iloc[val_ids, :], y.iloc[val_ids]\n",
    "        print('train shapes:', X_train.shape, len(y_train))\n",
    "        print('val shapes:', X_val.shape, len(y_val))\n",
    "        train_lgb = lgb.Dataset(X_train, label=y_train, categorical_feature=CAT_COLS, free_raw_data=False)\n",
    "        val_lgb = lgb.Dataset(X_val, label=y_val, categorical_feature=CAT_COLS, free_raw_data=False)\n",
    "        del X_train, y_train, X_val, y_val\n",
    "        gc.collect()\n",
    "        model = lgb.train(lgb_params, train_lgb, valid_sets=[val_lgb], verbose_eval=200) \n",
    "        model_file = '{}/model_{}_store_{}_fold_{}.lgb'.format(MODELS_DIR, MODEL_VER, store_id, fold_num)\n",
    "        model.save_model(model_file)\n",
    "        print('save to file:', model_file)\n",
    "    del X, y, train_lgb, val_lgb\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id = 0\n",
    "models_by_folds = []\n",
    "feat_importances_by_folds = []\n",
    "for fold_num in range(N_FOLDS):\n",
    "    model_file = '{}/model_{}_store_{}_fold_{}.lgb'.format(MODELS_DIR, MODEL_VER, store_id, fold_num)\n",
    "    model = lgb.Booster(model_file=model_file)\n",
    "    models_by_folds.append(model)\n",
    "    feat_importances_by_folds.append(model.feature_importance())\n",
    "    print('loaded:', model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = sorted(\n",
    "    [(f, v) for f, v in zip(train_cols, np.mean(feat_importances_by_folds, axis=0))],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "labels, values = [x[0] for x in feat_importances[:20]], [x[1] for x in feat_importances[:20]]\n",
    "y_pos = np.arange(len(labels))\n",
    "ax.barh(y_pos, values)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Performance')\n",
    "ax.set_title('feature importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spred = strain[\n",
    "    (strain['d'] > (END_D - PRED_FWD)) &\n",
    "    (strain['d'] <= END_D) &\n",
    "    (strain['store_id'] == store_id)\n",
    "].copy()\n",
    "preds_by_folds = []\n",
    "for model in tqdm(models_by_folds):\n",
    "    preds = model.predict(spred[train_cols])\n",
    "    preds_by_folds.append(preds)\n",
    "preds = np.mean(preds_by_folds, axis=0)\n",
    "print(len(preds))\n",
    "spred.loc[:, 'sales'] = np.where(preds <= .5, 0, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_name = np.random.choice(strain[strain['store_id'] == store_id]['id'].unique())\n",
    "print('id to draw:', id_name)\n",
    "id_sales = strain.loc[\n",
    "    (strain['id'] == id_name) & \n",
    "    (strain['d'] > (END_D - 2 * PRED_FWD)) &\n",
    "    (strain['d'] <= END_D) &\n",
    "    (strain['store_id'] == store_id)\n",
    "].set_index('date')\n",
    "id_sales_pred = spred.loc[spred['id'] == id_name].set_index('date')\n",
    "plt.figure(figsize=(18, 4))\n",
    "id_sales['sales'].plot(label='sales')\n",
    "id_sales_pred['sales'].plot(label='sales pred')\n",
    "plt.title(id_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT AND SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for pred_d in tqdm(range(1, PRED_FWD + 1)):\n",
    "    print('=' * 10, 'forecast day forward:', pred_d, '=' * 10) \n",
    "    strain = make_features_last_sales(strain)\n",
    "    spred = strain[strain['d'] > (END_D - 100)].copy()\n",
    "    spred = make_features_win(spred)\n",
    "    for store_id in store_ids:\n",
    "        preds_by_folds = []\n",
    "        for fold_num in range(N_FOLDS):\n",
    "            model_file = '{}/model_{}_store_{}_fold_{}.lgb'.format(MODELS_DIR, MODEL_VER, store_id, fold_num)\n",
    "            model = lgb.Booster(model_file=model_file)\n",
    "            preds = model.predict(\n",
    "                spred.loc[\n",
    "                    (spred['d'] == (END_D + pred_d)) & (spred['store_id'] == store_id), \n",
    "                    train_cols\n",
    "                ]\n",
    "            )\n",
    "            preds_by_folds.append(preds)\n",
    "            print('store predicted:', store_id, '| model:', model_file)\n",
    "        preds = np.mean(preds_by_folds, axis=0)\n",
    "        strain.loc[\n",
    "            (strain['d'] == (END_D + pred_d)) & (strain['store_id'] == store_id), \n",
    "            'sales'\n",
    "        ] = np.where(preds < .1, 0, preds)\n",
    "        print('store predicted all folds:', store_id)   \n",
    "    all_sales = strain[strain['d'] == (END_D + pred_d)]['sales'].values\n",
    "    print('day forward:', END_D + pred_d, 'all sales:', np.sum(all_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name = np.random.choice(strain['id'].unique())\n",
    "id_sales = strain.loc[\n",
    "    (strain['id'] == id_name) & \n",
    "    (strain['d'] > (END_D - 2 * PRED_FWD)) &\n",
    "    (strain['d'] <= END_D)\n",
    "].set_index('date')\n",
    "id_sales_pred = strain.loc[\n",
    "    (strain['id'] == id_name) & \n",
    "    (strain['d'] > END_D)\n",
    "].set_index('date')\n",
    "plt.figure(figsize=(18, 4))\n",
    "id_sales['sales'].plot(label='sales')\n",
    "id_sales_pred['sales'].plot(label='sales prediction')\n",
    "plt.title(id_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spred_subm = strain.loc[strain['d'] > END_D, ['id', 'd', 'sales']].copy()\n",
    "spred_subm['d'] = spred_subm['d'].apply(lambda x: 'F{}'.format(x - END_D))\n",
    "spred_subm.loc[spred_subm['sales'] < 0, 'sales'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = ['F{}'.format(x) for x in range(1, PRED_FWD + 1)]\n",
    "spred_subm = spred_subm.set_index(['id', 'd']).unstack()['sales'][f_cols].reset_index()\n",
    "spred_subm.fillna(0, inplace=True)\n",
    "spred_subm.sort_values('id', inplace=True)\n",
    "spred_subm.reset_index(drop=True, inplace=True)\n",
    "spred_subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spred_subm_eval = spred_subm.copy()\n",
    "spred_subm_eval['id'] = spred_subm_eval['id'].str.replace('validation', 'evaluation')\n",
    "spred_subm = pd.concat([spred_subm, spred_subm_eval], axis=0, sort=False)\n",
    "spred_subm.reset_index(drop=True, inplace=True)\n",
    "spred_subm.to_csv('submission.csv', index=False)\n",
    "print('submission saved:', spred_subm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMSSEEvaluator(object):\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "        train_df['all_id'] = 0  # for lv1 aggregation\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "        weight_df = self.get_weight_df()\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'state_id',\n",
    "            'store_id',\n",
    "            'cat_id',\n",
    "            'dept_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            'item_id',\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            scale = []\n",
    "            for _, row in train_y.iterrows():\n",
    "                series = row.values[np.argmax(row.values != 0):]\n",
    "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = getattr(self, f'lv{lv}_scale')\n",
    "        return (score / scale).map(np.sqrt)\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "        group_ids = []\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n",
    "            group_ids.append(group_id)\n",
    "            all_scores.append(lv_scores.sum())\n",
    "        return group_ids, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = pd.read_csv('{}/sales_train_evaluation.csv'.format('./data_upd'))\n",
    "df_calendar = pd.read_csv('{}/calendar.csv'.format(DATA_DIR))\n",
    "df_prices = pd.read_csv('{}/sell_prices.csv'.format(DATA_DIR))\n",
    "df_sample_submission = pd.read_csv('{}/sample_submission.csv'.format(DATA_DIR))\n",
    "df_sample_submission['order'] = range(df_sample_submission.shape[0])\n",
    "df_train = df_train_full.iloc[:, :-28]\n",
    "df_valid = df_train_full.iloc[:, -28:]\n",
    "evaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.read_csv('submission.csv'.format(DATA_DIR))\n",
    "preds_valid = y_pred[y_pred.id.str.contains('validation')]\n",
    "preds_valid = preds_valid.merge(\n",
    "    df_sample_submission[['id', 'order']], \n",
    "    on='id'\n",
    ").sort_values('order').drop(\n",
    "    ['id', 'order'], \n",
    "    axis=1\n",
    ")\n",
    "columns = {}\n",
    "for i in range(PRED_FWD):\n",
    "    columns.update({'F{}'.format(i + 1): 'd_{}'.format(END_D + i + 1)})\n",
    "print('rename columns:', columns)\n",
    "preds_valid.rename(columns=columns, inplace=True)\n",
    "preds_valid.reset_index(inplace=True)\n",
    "preds_valid.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, scores = evaluator.score(preds_valid)\n",
    "score_public_lb = np.mean(scores)\n",
    "for i in range(len(groups)):\n",
    "    print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n",
    "print(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
